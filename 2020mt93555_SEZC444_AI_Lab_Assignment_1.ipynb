{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Lab Assignment 1\n",
    "\n",
    "- Course: **SEZC444**\n",
    "- Student ID: **2020mt93555**\n",
    "- Student name : **Paul Cleetus**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A** Implement the Learning Agent which is in search of the only gold box in the given static environment equipped with the metal detectors . The metal detectors are capable of sensing the metal in their own and all surrounding cells in their circumference.\n",
    "\n",
    "While the agent initial battery is at 10000 points. Each movement of the agent reduces the battery by 1 point and at a time the agent can move only one cell either of the forward, backward, left or right directions. If the agent moves in to metal detector sensing range, the agentâ€™s battery life will reduce by 100 battery points. The ultimate goal is to catch the gold box and it results 10000 additional battery points. The agent can sense only its battery points, which are automatically updated on each movement and the resultant cell contents. \n",
    "```\n",
    "7 marks \n",
    "1 mark for mapping to correct concept (among our topics), \n",
    "1 mark for briefing the learning parameters and \n",
    "5 marks for an error-free executing model\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEAS Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Measure**\n",
    "- Each Movement : -1 point \n",
    "- Adjescant to Metal Detector : -100 points\n",
    "- Gold Box found : +10000 points \n",
    "\n",
    "**Environment**\n",
    "- A 10 x 10 grid of rooms denoted Rows 1 to 10, Columns A to J\n",
    "- Initial Battery : 10000 points \n",
    "\n",
    "**Actuators**\n",
    "- Agent can move 1 step Forward / Backward / Left / Right\n",
    "- If the movement is againt a wall then there is no effect \n",
    "\n",
    "**Sensor**\n",
    "The agent can sense only its battery points, which are automatically updated on each movement and the resultant cell contents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the **Q-Learning** Re-inforcement learning technique to solve this problem .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from pprint import pprint\n",
    "\n",
    "# Define the Row and Column range for the environmnet\n",
    "rows    = np.arange(1,11)\n",
    "columns = list(string.ascii_uppercase[0:10])\n",
    "\n",
    "# Specify the colours for various elements in environment\n",
    "gold     = 4.7\n",
    "detector = 3.9\n",
    "magfield = 2.2\n",
    "agent    = 5.5\n",
    "empty    = 0.0\n",
    "temp     = 0.0\n",
    "\n",
    "# Specify the text annotations to be plotted in the output for each element\n",
    "notation = {\n",
    "    gold     : 'G',\n",
    "    detector : 'D',\n",
    "    magfield : 'X',\n",
    "    agent    : 'A',\n",
    "    empty    : '',\n",
    "}\n",
    "\n",
    "# Specify the performance measures\n",
    "battery        = 10000\n",
    "reward   = 0\n",
    "feedback = {\n",
    "    gold     : 10000,\n",
    "    detector :  -100,\n",
    "    magfield :  -100,\n",
    "    agent    :     0,\n",
    "    empty    :    -1,\n",
    "    'wall'   :  -100,\n",
    "}\n",
    "\n",
    "# define actions\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "# Some Global Variables for the environment\n",
    "agent_location = []\n",
    "next_location = []\n",
    "agent_stop = False\n",
    "environment = np.array([])\n",
    "\n",
    "\n",
    "#define training parameters\n",
    "epsilon = 0.95 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.95 #discount factor for future rewards\n",
    "learning_rate = 0.95 #the rate at which the AI agent should learn\n",
    "\n",
    "q_values = np.zeros((10, 10, 4)) #the q-table for 10x10 matrix and 4 actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize the Matrix environment as given in the question\n",
    "def createEnvironment():\n",
    "    global agent_location\n",
    "    global next_location\n",
    "    global agent_stop\n",
    "    global environment\n",
    "    \n",
    "    # agent location\n",
    "    agent_location = [9,0]\n",
    "    next_location  = [9,0]\n",
    "    #next_location  = agent_location.copy()\n",
    "\n",
    "    # agent can stop\n",
    "    agent_stop  = False\n",
    "\n",
    "    # Specify the environmnet as given in the Problem Statement for the assignment\n",
    "    environment = np.array(\n",
    "                    [[empty, empty, empty, empty, empty, empty, empty, magfield, magfield, magfield],    # 1\n",
    "                     [empty, empty, magfield, magfield, magfield, empty, empty, magfield, detector, magfield],    # 2\n",
    "                     [empty, empty, magfield, detector, magfield, empty, magfield, magfield, magfield, magfield],    # 3\n",
    "                     [magfield, magfield, magfield, magfield, magfield, empty, magfield, detector, magfield, empty],    # 4\n",
    "                     [magfield, detector, magfield, magfield, magfield, empty, magfield, magfield, magfield, empty],    # 5\n",
    "                     [magfield, magfield,magfield, detector, magfield, empty, empty, empty, empty, empty],    # 6\n",
    "                     [empty, empty, magfield, magfield, magfield, empty, empty, empty, empty, empty],    # 7\n",
    "                     [empty, empty, empty, empty, empty, empty, magfield, magfield, magfield, empty],    # 8\n",
    "                     [empty, magfield, magfield, magfield, empty, empty, magfield, detector, magfield, empty],    # 9\n",
    "                     [agent, magfield, detector, magfield, empty, empty, magfield, magfield, magfield, gold]] #10\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the matrix plot\n",
    "def displayPlot(environment):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(environment)\n",
    "\n",
    "    # Show all rooms\n",
    "    ax.set_xticks(np.arange(len(columns)))\n",
    "    ax.set_yticks(np.arange(len(rows)))\n",
    "\n",
    "    # Label with the respective list entries\n",
    "    ax.set_xticklabels(columns)\n",
    "    ax.set_yticklabels(rows)\n",
    "\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(rows)):\n",
    "        for j in range(len(columns)):\n",
    "            text = ax.text(j, i, notation.get(environment[i, j]),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Q-Learning Agent searching the Gold box among metal detector array\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the agent if not hitting the wall\n",
    "def moveAgent():\n",
    "\n",
    "#    if moveDirection == \"left\" and next_location[1] > 0:\n",
    "#        next_location[1] = next_location[1] - 1\n",
    "#    elif moveDirection == \"right\" and next_location[1] < len(columns) - 1:\n",
    "#        next_location[1] = next_location[1] +  1\n",
    "#    elif moveDirection == \"forward\" and next_location[0] > 0:\n",
    "#        next_location[0] = next_location[0] - 1\n",
    "#    elif moveDirection == \"backward\" and next_location[0] < len(rows) - 1:\n",
    "#        next_location[0] = next_location[0] + 1\n",
    "        \n",
    "    global temp\n",
    "    global agent_location\n",
    "    global next_location\n",
    "    \n",
    "    updateBattery()\n",
    "    \n",
    "    # swap colours\n",
    "    environment[agent_location[0]][agent_location[1]] = temp\n",
    "    temp = environment[next_location[0]][next_location[1]]\n",
    "    environment[next_location[0]][next_location[1]] = agent\n",
    "    agent_location = next_location.copy()\n",
    "    \n",
    "    displayPlot(environment)\n",
    "    \n",
    "    \n",
    "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "    #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
    "    #then choose the most promising value from the Q-table for this state.\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[current_row_index,current_column_index])\n",
    "    else: #choose a random action\n",
    "        return np.random.randint(4)\n",
    "\n",
    "#define a function that will get the next location based on the chosen action\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 'up' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "    elif actions[action_index] == 'right' and current_column_index < len(columns) - 1:\n",
    "        new_column_index += 1\n",
    "    elif actions[action_index] == 'down' and current_row_index <  len(rows) - 1:\n",
    "        new_row_index += 1\n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1  \n",
    "    return new_row_index, new_column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the batter points based on the next position feedback\n",
    "def updateBattery():\n",
    "    global battery\n",
    "    global reward\n",
    "    global agent_stop\n",
    "    \n",
    "    if next_location[0] == agent_location[0] and next_location[1] == agent_location[1]:\n",
    "        print(\"Agent has hit a wall\")\n",
    "        reward = feedback['wall']\n",
    "    else:\n",
    "        print(\"Next Environment : \", notation[environment[next_location[0]][next_location[1]]])\n",
    "        reward = feedback[environment[next_location[0]][next_location[1]]]\n",
    "        \n",
    "    print(\"Feedback : \", reward)\n",
    "    battery = battery + reward\n",
    "    \n",
    "    \n",
    "    if reward == 10000:\n",
    "        print(\"Agent has found the Gold\")\n",
    "        agent_stop  = True\n",
    "    elif battery < 1:\n",
    "        print(\"Agent has drained the battery\")\n",
    "        agent_stop  = True\n",
    "        \n",
    "    print(\"Agent Battery Level : \" + str(battery))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the intial state and status for reference\n",
    "createEnvironment()\n",
    "\n",
    "print(\"This cell output initial view of the learning agent environment and Q-values table\")\n",
    "\n",
    "displayPlot(environment)\n",
    "pprint(q_values)\n",
    "\n",
    "print(\"Agent Battery Level : \" + str(battery))\n",
    "print(\"Feedback : \" + str(reward))\n",
    "print(\"See the Next cell output to view the learning progress step snapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training parameters\n",
    "global epsilon\n",
    "global discount_factor\n",
    "global learning_rate\n",
    "global agent_stop\n",
    "global q_values\n",
    "global reward\n",
    "global hit_wall\n",
    "global battery\n",
    "\n",
    "#run through 5 training episodes\n",
    "for episode in range(5):\n",
    "    #get the starting location for this episode\n",
    "    row_index, column_index = 9 , 0\n",
    "    hit_wall   = False\n",
    "    agent_stop = False\n",
    "    battery    = 10000\n",
    "    print('Episode : ', episode)\n",
    "    \n",
    "    pprint(q_values)\n",
    "    \n",
    "    #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "    #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
    "    while not agent_stop:\n",
    "\n",
    "        #choose which action to take (i.e., where to move next)\n",
    "        action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "        #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "        old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "        row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "            \n",
    "        next_location[0] = row_index\n",
    "        next_location[1] = column_index\n",
    "        moveAgent()\n",
    "    \n",
    "        #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "        old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "        #update the Q-value for the previous state and action pair\n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "        q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Agent's Final position\n",
    "global environment\n",
    "global q_values\n",
    "\n",
    "displayPlot(environment)\n",
    "        \n",
    "pprint(q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B** Adjust the learning parameters for five times and return the best of those five parameter sets in terms of quick convergence towards the solution \n",
    "\n",
    "*3 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Learning parameters:\n",
    "\n",
    "**First run parameters** the followng parameters where used\n",
    "```\n",
    "epsilon = 0.5 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
    "```\n",
    "\n",
    "- The Gold was found first time in 2nd Episode of training iteration with around 5144 battery points remaining\n",
    "- The Gold was found second time in 4th Episode of training iteration with around 7350 battery points remaining\n",
    "- The Gold was found third time in 5th Episode of training iteration with around 5289 battery points remaining\n",
    "\n",
    "**Second run parameters** the followng parameters where used\n",
    "```\n",
    "epsilon = 0.7 \n",
    "discount_factor = 0.9 \n",
    "learning_rate = 0.5 \n",
    "```\n",
    "\n",
    "- The Gold was found first time in 3rd Episode of training iteration with around 4204 battery points remaining\n",
    "- The Gold was found second time in 4th Episode of training iteration with around 9370 battery points remaining\n",
    "\n",
    "**Third run parameters** the followng parameters where used\n",
    "```\n",
    "epsilon = 0.85 \n",
    "discount_factor = 0.9 \n",
    "learning_rate = 0.95 \n",
    "```\n",
    "- The Gold was found first time in 1st Episode of training iteration with around 1287 battery points remaining\n",
    "- The Gold was found second time in 2nd Episode of training iteration with around 6757 battery points remaining\n",
    "- The Gold was found third time in 3rd Episode of training iteration with around 6509 battery points remaining\n",
    "- The Gold was found fourth time in 4th Episode of training iteration with around 8818 battery points remaining\n",
    "\n",
    "\n",
    "**Fourth run parameters** the followng parameters where used\n",
    "```\n",
    "epsilon = 0.95 \n",
    "discount_factor = 0.5 \n",
    "learning_rate = 0.95 \n",
    "```\n",
    "- The Gold was found first time in 1st Episode of training iteration with around 5053 battery points remaining\n",
    "- The Gold was found second time in 2nd Episode of training iteration with around 5653 battery points remaining\n",
    "- The Gold was found third time in 3rd Episode of training iteration with around 8485 battery points remaining\n",
    "- The Gold was found fourth time in 4th Episode of training iteration with around 5651 battery points remaining\n",
    "\n",
    "**Fifth run parameters** the followng parameters where used\n",
    "```\n",
    "epsilon = 0.95 \n",
    "discount_factor = 0.95 \n",
    "learning_rate = 0.95 \n",
    "```\n",
    "- The Gold was found first time in 2nd Episode of training iteration with around 8529 battery points remaining\n",
    "- The Gold was found second time in 3rd Episode of training iteration with around 6650 battery points remaining\n",
    "- The Gold was found third time in 4th Episode of training iteration with around 9054 battery points remaining\n",
    "\n",
    "\n",
    "**Additonal feedback than the ones suggested in question:**\n",
    "I experimented with adding in a feedback for encountering a wall,\n",
    "so that the q-values for such action could also be learned by the agent as a punishment / -ve feedback\n",
    "This considerably reduced the learning time and avoided repeating the decision to move into the wall always\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Creating Annotated Heatmap in Matplotlib](https://matplotlib.org/stable/gallery/images_contours_and_fields/image_annotated_heatmap.html#sphx-glr-gallery-images-contours-and-fields-image-annotated-heatmap-py)\n",
    "-[Video : Q-Learning: A Complete Example in Python, Dr. Daniel Soper](https://www.youtube.com/watch?v=iKdlKYG78j4)\n",
    "-[Reference Code: Q-Learning: A Complete Example in Python, Dr. Daniel Soper](https://colab.research.google.com/drive/1E2RViy7xmor0mhqskZV14_NUj2jMpJz3#scrollTo=z43QX_t080q3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
